# AWS Configuration
SAGEMAKER_ROLE_ARN=arn:aws:iam::326614947732:role/AmazonSageMakerFullAccess
S3_BUCKET=amazon-sagemaker-326614947732-us-east-1-b6aed9d1f258
AWS_REGION=us-east-1

# API Keys
HUGGING_FACE_HUB_TOKEN=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
WANDB_API_KEY=c81320346d825ecba691cbc52468fbb48b97e834

# Model Configuration
BASE_MODEL_NAME=meta-llama/Llama-3.2-3B-Instruct
TRAINING_INSTANCE_TYPE=ml.p4d.24xlarge
INFERENCE_INSTANCE_TYPE=ml.g5.2xlarge

# Training Hyperparameters
NUM_EPOCHS=0.1
BATCH_SIZE=2
LEARNING_RATE=2e-4
